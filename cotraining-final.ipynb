{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extracttext(myfile):\n",
    "    soup = BeautifulSoup(myfile,\"lxml\")\n",
    "    # kill all script and style elements\n",
    "    for script in soup([\"script\", \"style\"]):\n",
    "        script.extract()    # rip it out\n",
    "\n",
    "    # get text\n",
    "    text = soup.get_text()\n",
    "\n",
    "    # break into lines and remove leading and trailing space on each\n",
    "    lines = (line.strip() for line in text.splitlines())\n",
    "    # break multi-headlines into a line each\n",
    "    chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "    # drop blank lines\n",
    "    text = '\\n'.join(chunk for chunk in chunks if chunk)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#http://stackoverflow.com/questions/10098533/implementing-bag-of-words-naive-bayes-classifier-in-nltk\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extracttext(myfile):\n",
    "    soup = BeautifulSoup(myfile,\"lxml\")\n",
    "    # kill all script and style elements\n",
    "    for script in soup([\"script\", \"style\"]):\n",
    "        script.extract()    # rip it out\n",
    "\n",
    "    # get text\n",
    "    text = soup.get_text()\n",
    "\n",
    "    # break into lines and remove leading and trailing space on each\n",
    "    lines = (line.strip() for line in text.splitlines())\n",
    "    # break multi-headlines into a line each\n",
    "    chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "    # drop blank lines\n",
    "    text = '\\n'.join(chunk for chunk in chunks if chunk)\n",
    "\n",
    "    return text\n",
    "\n",
    "import os\n",
    "messages_html = []\n",
    "labels_html = []\n",
    "for root, dirs, files in os.walk('D:/Downloads/course-cotrain-data/fulltext'):\n",
    "    for name in files:\n",
    "            f = open(os.path.join(root, name))\n",
    "            messages_html.append(extracttext(f))\n",
    "            labels_html.append(root.split('\\\\')[-1])\n",
    "\n",
    "# NOTE: HERE MAKING HUGE ASSUMPTION THAT BOTH VIEWS OF A SINGLE WEBPAGE\n",
    "#(I.E HTML AND INLINK APPEAR IN THE SAME ORDER IN THE RESPECTIVE FULLTEXT AND INLINKS FOLDER)\n",
    "messages_inlinks = []\n",
    "labels_inlinks = []\n",
    "for root, dirs, files in os.walk('D:/Downloads/course-cotrain-data/inlinks'):\n",
    "    for name in files:\n",
    "            f = open(os.path.join(root, name))\n",
    "            messages_inlinks.append(extracttext(f))\n",
    "            labels_inlinks.append(root.split('\\\\')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#bing: bag of words nltk python\n",
    "#http://stackoverflow.com/questions/15507172/how-to-get-bag-of-words-from-textual-data\n",
    "#http://radimrehurek.com/data_science_python/\n",
    "from nltk.corpus import stopwords\n",
    "def split_into_lemmas(message):\n",
    "    #message = unicode(message, 'utf8').lower()\n",
    "    message = message.lower() #In our case, message already unicode\n",
    "    words = TextBlob(message).words\n",
    "    # for each word, take its \"base form\" = lemma \n",
    "    return [word.lemma for word in words if word not in stopwords.words('english')]\n",
    "\n",
    "#messages.message.head().apply(split_into_lemmas)\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer\n",
    "from textblob import TextBlob\n",
    "\n",
    "def get_processed_data(messages):\n",
    "    bow_transformer = CountVectorizer(analyzer=split_into_lemmas).fit(messages)\n",
    "    messages_bow = bow_transformer.transform(messages)\n",
    "    tfidf_transformer = TfidfTransformer().fit(messages_bow)\n",
    "    messages_tfidf = tfidf_transformer.transform(messages_bow)\n",
    "    return bow_transformer,messages_bow,tfidf_transformer,messages_tfidf\n",
    "\n",
    "a1,b1,c1,messages1_tfidf = get_processed_data(messages_html)\n",
    "a2,b2,c2,messages2_tfidf = get_processed_data(messages_inlinks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(labels_html,open(\"D:/Downloads/course-cotrain-data/saved-objects/labels_html.p\", \"wb\" ))\n",
    "pickle.dump(labels_inlinks,open(\"D:/Downloads/course-cotrain-data/saved-objects/labels_inlinks.p\", \"wb\" ))\n",
    "\n",
    "pickle.dump( a1, open( \"D:/Downloads/course-cotrain-data/saved-objects/a1.p\", \"wb\" ) )\n",
    "pickle.dump( b1, open( \"D:/Downloads/course-cotrain-data/saved-objects/b1.p\", \"wb\" ) )\n",
    "pickle.dump( c1, open( \"D:/Downloads/course-cotrain-data/saved-objects/c1.p\", \"wb\" ) )\n",
    "pickle.dump( messages1_tfidf, open( \"D:/Downloads/course-cotrain-data/saved-objects/messages1_tfidf.p\", \"wb\" ) )\n",
    "pickle.dump( a2, open( \"D:/Downloads/course-cotrain-data/saved-objects/a2.p\", \"wb\" ) )\n",
    "pickle.dump( b2, open( \"D:/Downloads/course-cotrain-data/saved-objects/b2.p\", \"wb\" ) )\n",
    "pickle.dump( c2, open( \"D:/Downloads/course-cotrain-data/saved-objects/c2.p\", \"wb\" ) )\n",
    "pickle.dump( messages2_tfidf, open( \"D:/Downloads/course-cotrain-data/saved-objects/messages2_tfidf.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "from nltk.corpus import stopwords\n",
    "def split_into_lemmas(message):\n",
    "    #message = unicode(message, 'utf8').lower()\n",
    "    message = message.lower() #In our case, message already unicode\n",
    "    words = TextBlob(message).words\n",
    "    # for each word, take its \"base form\" = lemma \n",
    "    return [word.lemma for word in words if word not in stopwords.words('english')]\n",
    "\n",
    "import pickle\n",
    "labels_html = pickle.load( open( \"D:/Downloads/course-cotrain-data/saved-objects/labels_html.p\", \"rb\" ) )\n",
    "labels_inlinks = pickle.load( open( \"D:/Downloads/course-cotrain-data/saved-objects/labels_inlinks.p\", \"rb\" ) )\n",
    "\n",
    "a1 = pickle.load( open( \"D:/Downloads/course-cotrain-data/saved-objects/a1.p\", \"rb\" ) )\n",
    "b1 = pickle.load( open( \"D:/Downloads/course-cotrain-data/saved-objects/b1.p\", \"rb\" ) )\n",
    "c1 = pickle.load( open( \"D:/Downloads/course-cotrain-data/saved-objects/c1.p\", \"rb\" ) )\n",
    "messages1_tfidf = pickle.load( open( \"D:/Downloads/course-cotrain-data/saved-objects/messages1_tfidf.p\", \"rb\" ) )\n",
    "a2 = pickle.load( open( \"D:/Downloads/course-cotrain-data/saved-objects/a2.p\", \"rb\" ) )\n",
    "b2 = pickle.load( open( \"D:/Downloads/course-cotrain-data/saved-objects/b2.p\", \"rb\" ) )\n",
    "c2 = pickle.load( open( \"D:/Downloads/course-cotrain-data/saved-objects/c2.p\", \"rb\" ) )\n",
    "messages2_tfidf = pickle.load( open( \"D:/Downloads/course-cotrain-data/saved-objects/messages2_tfidf.p\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'> (1051, 20529)\n",
      "<class 'scipy.sparse.csr.csr_matrix'> (1051, 2014)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "m1 = messages1_tfidf\n",
    "m2 = messages2_tfidf\n",
    "print type(m1),m1.shape\n",
    "print type(m2),m2.shape\n",
    "plt.spy(m1[:,:100])\n",
    "plt.ylabel('different webpages')\n",
    "plt.xlabel('word-frequency')\n",
    "plt.title('Bag of Words representation of the corpus')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(788, 20529) (263, 20529) HTML-view for train-test respectively\n",
      "(788, 2014) (263, 2014) INLINK-view for train-test respectively\n",
      "(4, 20529)\n",
      "['course' 'non-course' 'non-course' 'non-course']\n",
      "(784, 20529)\n",
      "(4, 2014)\n",
      "['course' 'non-course' 'non-course' 'non-course']\n",
      "(784, 2014)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "labels_html = np.array(labels_html)\n",
    "labels_inlinks = np.array(labels_inlinks)\n",
    "\n",
    "#Splitting Data into train-test set\n",
    "TESTSIZE = 0.25\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=TESTSIZE) #n_splits is redundant parameter here\n",
    "for train_index, test_index in sss.split(messages1_tfidf, labels_html):\n",
    "\n",
    "    X1_train, X1_test = messages1_tfidf[train_index], messages1_tfidf[test_index]\n",
    "    y1_train, y1_test = labels_html[train_index], labels_html[test_index]\n",
    "    \n",
    "    #Splitting with same-index (important to not change indexes across HTML and Inlinks)\n",
    "    X2_train, X2_test = messages2_tfidf[train_index], messages2_tfidf[test_index]\n",
    "    y2_train, y2_test = labels_inlinks[train_index], labels_inlinks[test_index]\n",
    "\n",
    "print X1_train.shape,X1_test.shape,'HTML-view for train-test respectively'\n",
    "print X2_train.shape,X2_test.shape,'INLINK-view for train-test respectively'\n",
    "#Selecting 12 points randomly as initial supervised examples\n",
    "sss = StratifiedShuffleSplit(n_splits=1, train_size=4,test_size=X1_train.shape[0]-4)\n",
    "for train_index, test_index in sss.split(X1_train, y1_train): #should be same if using X2,y2\n",
    "    \n",
    "    #Intial labeled-data\n",
    "    X1_train_init,y1_train_init = X1_train[train_index],y1_train[train_index]\n",
    "    X2_train_init,y2_train_init = X2_train[train_index],y2_train[train_index]\n",
    "\n",
    "    #Intial unlabeled-corpus U (discarding labels y1,y2)\n",
    "    X1_unlabeled = X1_train[test_index]\n",
    "    X2_unlabeled = X2_train[test_index]\n",
    "\n",
    "\n",
    "print X1_train_init.shape\n",
    "print y1_train_init\n",
    "print X1_unlabeled.shape\n",
    "print X2_train_init.shape\n",
    "print y2_train_init #should be same as y1\n",
    "print X2_unlabeled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k is 500 u is 150\n",
      "['course' 'non-course']\n",
      "(784, 20529) (784, 2014)\n",
      "501 501\n"
     ]
    }
   ],
   "source": [
    "def gen_new_data(h,X_unlabeled,p,n):\n",
    "\n",
    "    X_new = []\n",
    "    y_new = []\n",
    "    ## REDUNDANT CODE (TRY TO IMPROVE)\n",
    "    h_col_1 = np.where(h.classes_=='course')[0][0] #the target class, positive examples\n",
    "    h_col_2 = np.where(h.classes_=='non-course')[0][0] #negative examples\n",
    "    ######\n",
    "    \n",
    "    a = h.predict_proba(X_unlabeled)\n",
    "    #print a\n",
    "    p_indices = a[:,h_col_1].argsort()[::-1]\n",
    "    n_indices = a[:,h_col_2].argsort()[::-1]\n",
    "    \n",
    "    return p_indices[:p],n_indices[:n]\n",
    "\n",
    "\n",
    "#Cotraining-algorithm\n",
    "def cotraining(p=1,n=3,u=75,k=35):\n",
    "    print 'k is',k, 'u is',u\n",
    "\n",
    "    from sklearn.naive_bayes import MultinomialNB\n",
    "    h1 = MultinomialNB().fit(X1_train_init,y1_train_init)\n",
    "    h2 = MultinomialNB().fit(X2_train_init,y2_train_init)\n",
    "\n",
    "    print h1.classes_\n",
    "\n",
    "\n",
    "\n",
    "    ##MAIN CODE\n",
    "    from sklearn.utils import resample\n",
    "    from scipy.sparse import vstack\n",
    "    print X1_unlabeled.shape,X2_unlabeled.shape\n",
    "    indices = range(X2_unlabeled.shape[0]) #should be same if we use X1\n",
    "\n",
    "    acc1=[h1.score(X1_test,y1_test)]\n",
    "    acc2=[h2.score(X2_test,y2_test)]\n",
    "    counter=0\n",
    "    while counter < k:\n",
    "        U_prime = resample(indices,n_samples=u)\n",
    "        temp1 = X1_unlabeled[U_prime]\n",
    "        temp2 = X2_unlabeled[U_prime]\n",
    "    \n",
    "        #generating new-labels(actually indices corresponding to labels)\n",
    "        p_indices1,n_indices1 = gen_new_data(h1,temp1,p,n)\n",
    "        p_indices2,n_indices2 = gen_new_data(h2,temp2,p,n)\n",
    "\n",
    "        #fitting new-datapoints\n",
    "        h1.partial_fit(vstack((temp1[p_indices2],temp1[n_indices2])),['course']*p+['non-course']*n)\n",
    "        h2.partial_fit(vstack((temp2[p_indices1],temp2[n_indices1])),['course']*p+['non-course']*n)\n",
    "    \n",
    "        #Keeping track of accuracy\n",
    "        acc1.append(h1.score(X1_test,y1_test))\n",
    "        acc2.append(h2.score(X2_test,y2_test))\n",
    "        counter+=1\n",
    "    return acc1,acc2\n",
    "\n",
    "acc1,acc2 = cotraining(u=150,k=500)\n",
    "print len(acc1),len(acc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "indexing = range(len(acc1))\n",
    "hnd1, = plt.plot(indexing,acc1,'b',label='HTML view')\n",
    "hnd2, =plt.plot(indexing,acc2,'g',label='IN-LINK view')\n",
    "\n",
    "#To add parameter-values\n",
    "extraString = \"p = \"+str(1)+\" n = \"+str(3)+\" u = \"+str(300)+\" k = \"+str(500)\n",
    "handles, labels = plt.gca().get_legend_handles_labels()\n",
    "handles.append(mpatches.Patch(color='none', label=extraString))\n",
    "plt.legend(handles=handles,loc=0)\n",
    "\n",
    "\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Co-Training iterations')\n",
    "plt.title('Accuracy versus number of iterations for one run of co-training experiment.')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
